
0. 学习的目的。
所谓神经网络的训练或者是学习，其主要目的在于通过学习算法得到神经网络解决指定问题所需的参数，这里的参数包括各层神经元之间的连接权重以及偏置等。


链接：
http://www.cnblogs.com/maybe2030/p/5597716.html

1. 监督学习算法和无监督学习算法(Supervised and Unsupervised):
有监督学习算法是由算法设计者将包含结果的数据交由算法进行处理，即需要用户明确的告诉算法需要做什么;
无监督学习算法是由程序自己进行学习。
链接：https://www.cnblogs.com/nowornever-L/p/6862278.html

2. 损失函数(Cost function):
    损失函数是用来估量模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。



svm损失函数：
https://www.cnblogs.com/guoyaohua/p/9436237.html

链接：
https://www.cnblogs.com/guoyaohua/p/9217206.html


3. 激活函数(Activation function)



4. 反向传播(backpropagation)
反向传播算法告诉我们当我们改变权值（weights）和偏置（biases）的时候，损失函数改变的速度和神经网络的整体表现。
反向传播可以理解为，在一个单向的前向传播结束后，会计算出对应的output值，计算output值与目标期望值直接的差距，然后通过链式法则，
计算并修改网络节点中的各个节点的权重值，重新向前进行前向传播继续计算一组output值，直到最终的output值与目标接近即达到最优，即总体误差最小，
反向传播也就是误差逆传播。




链接：

https://www.cnblogs.com/charlotte77/p/5629865.html
https://www.cnblogs.com/nowornever-L/p/6908944.html
https://www.imooc.com/article/44125
https://www.jianshu.com/p/2e02bc6384a8
https://www.cnblogs.com/charlotte77/p/5625984.html
https://www.cnblogs.com/charlotte77/p/5629865.html

5. 正则化(Regularization)

链接：
https://www.cnblogs.com/nowornever-L/p/6862320.html



6. 优化器算法(Optimization)
   a. 梯度下降(Gradient Descent)
   
   b.随机梯度下降(Stochastic Gradient Descent)
   
   c.小Batch 随机梯度下降(Mini Batch stochastic Gradient Descent)
   
   d.动量（momentum）
   
   e.adagrad



链接：
https://www.cnblogs.com/guoyaohua/p/8542554.html

7. 学习速度(Learning Rate)


8. 权重初始化(Weight Initiazation)
   a.All zero initialzation
   
   b.Initialzation with small random Numbers
   
   c. Calibrate the Variances

9. 输入层(Input Layer)



10. 隐藏层(Hidden Layers)


11. 神经元(Unit(Neurous))


12. Batch 归一化(Batch Normalization)
  Batch Normalization的目的是让网络训练过程中每一层神经网络的输入保持相同的分布。
  
链接：
http://www.cnblogs.com/guoyaohua/p/8724433.html


13. 逻辑回归(Logistic Regression)

链接：
https://www.cnblogs.com/nowornever-L/p/6878138.html
https://www.cnblogs.com/nowornever-L/p/6862314.html


14. 向前传播(ForwardPropagration)


https://www.cnblogs.com/charlotte77/p/5629865.html
15. 超参数
   由人工手动或者外界设置，用来帮助算法估计模型的参数，超参数是在开始学习过程之前设置值的参数，
   而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。

链接：
https://blog.csdn.net/weixin_43061687/article/details/82711430



16. 白化
白化，就是对输入数据分布变换到0均值，单位方差的正态分布。
图像白化（whitening）可用于对过度曝光或低曝光的图片进行处理，处理的方式就是改变图像的平均像素值为 0 ，改变图像的方差为单位方差 1。





17. 池化和池化层
    a. 池化（下采样）
    池化等同于图像处理中的PCF(Pixel compress filter)或者说是下采样。主要是用来对特征进行压缩用的。
    池化一般采用两种方式 ：
       average pooling：将一个filter区域用均值替代。
       max pooling:     将一个filter区域用最大值替代。
注：池化和卷积的区别在于池化是按filter大小进行步进的，而卷积是按pixel进行步进的。  

    
    b.池化层
     对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。
  
  链接：
  https://cloud.tencent.com/developer/article/1347909
  https://blog.csdn.net/hhello_jason/article/details/79071769
  https://blog.csdn.net/qq_40525008/article/details/79964306
  https://blog.csdn.net/qq_18435519/article/details/53234516
  https://blog.csdn.net/cheneykl/article/details/79873424?utm_source=blogxgwz4
  
18. 全连接层
    连接所有的特征，将输出值送给分类器。多层感知器叫做“全连接层”。
    卷积和池化层的输出表示了输入图像的高级特征。全连接层的目的是为了使用这些特征把输入图像基于训练数据集进行分类。
    从全连接层得到的输出概率和为 1。这个可以在输出层使用 softmax 作为激活函数进行保证。
    

19. 卷积层
   通过卷积运算，可以使原信号特征增强，并且降低噪音。在深度学习中做卷积的目的是为了从输入图像中提取特征（高通滤波器）。
在CNN术语中，卷积核称为特征检测器(feature detector),卷积后的矩阵或图片称之为卷积特征（convoluted feature）,
或者激活图(Activation map),或者特征图(feature map).即滤波器在原始输入图像上的作用是特征检测器。

 深度：在CNN中，深度是对应的是卷积操作所需的滤波器个数，即与传统图像处理不一样，深度学习中卷积，会使用多个不同的卷积核对图像进行卷积操作，
不同的卷积核都会生成一个对应的feature map. 其feature map的个数也就是卷积核的个数，也即是深度。

   
链接：
https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
http://www.hackcv.com/index.php/archives/104/?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io
https://www.cnblogs.com/guoyaohua/p/9443612.html

20. Feature Map
在每个卷积层，数据都是以三维形式存在的。你可以把它看成许多个二维图片叠在一起，其中每一个称为一个feature map。
在输入层，如果是灰度图片，那就只有一个feature map；如果是彩色图片，一般就是3个feature map（红绿蓝）。
层与层之间会有若干个卷积核（kernel），上一层和每个feature map跟每个卷积核做卷积，都会产生下一层的一个feature map。

链接：
https://blog.csdn.net/allenlzcoder/article/details/78739346


21. Dropout


22. 链式法则：
由基本函数组成的复杂组合的求导，就做链式法则。



链接：
https://blog.csdn.net/dev_csdn/article/details/78500708

23. 数据增强（data augmentation ）

链接：
https://blog.csdn.net/qq_17448289/article/details/78051703


24. 迁移学习 Transfer Learning（fine-tune）


链接：
https://blog.csdn.net/u013841196/article/details/80919857


25. K 邻近

链接：
https://coolshell.cn/articles/8052.html


26. 主成分分析方法 PCA(principal Component Analysis)
PCA将数据进行不同正交方向上的投影，使得数据在投影方向上的方差最大且其方向相互正交。
然后将方差最大的方向作为主要特征，并且各个正交方向上的数据分离其相关性。从而得到
不同正交方向上的数据，并进行分别处理，从而达到降低数据维度。



链接：
https://blog.csdn.net/hustqb/article/details/78394058
http://m.elecfans.com/article/594908.html
https://blog.csdn.net/baimafujinji/article/details/79376378
http://www.cnblogs.com/sowhat4999/p/5824880.html
https://blog.csdn.net/watkinsong/article/details/38536463
https://blog.csdn.net/u013719780/article/details/78352262
http://www.cnblogs.com/zy230530/p/7074215.html

27. 非线性操作(Rectified Linear Unit/ReLU)

ReLU 是一个元素级别的操作（应用到各个像素），并将卷积后的特征图中的所有小于 0 的像素值设置为零，其目的是引入非线性特征。





Other Basic Concept:
1. 深度学习利用正向传播来提取特征，同时利用反向传播来调整参数.
2. CNN叫做卷积神经网络，神经网络的神经元在计算时使用二维的卷积和的方式进行，使得运算在二维空间上进行扩展。
3. RNN叫做递归神经网络，递归神经网络则是在时间上进行扩展;递归神经网络是在时间上进行扩展，
即考虑当前问题的同时也要考虑上一个状态的结果作为当前状态的输入.CNN-用于图像处理，RNN用于语音处理.


some resource:
讲解比较好的：http://www.hackcv.com/index.php/archives/104/?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io
其他：
http://www.cnblogs.com/maybe2030/p/5597716.html
https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/
http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html
https://www.cnblogs.com/guoyaohua/p/8534077.html
https://www.cnblogs.com/nowornever-L/p/6890663.html
https://www.cnblogs.com/nowornever-L/p/6890648.html
https://www.cnblogs.com/wuxiangli/p/7258510.html
http://lamda.nju.edu.cn/weixs/slide/CNNTricks_slide.pdf

